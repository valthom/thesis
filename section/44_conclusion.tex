 \chapter*{Conclusion and future works}

\section*{Conclusion}
In this thesis, we have explored the roles and challenges presented by
stochasticity in reinforcement learning and optimization algorithms. We have
investigated the impact of noise in the learning process and presented four significant contributions that address various aspects of this issue.
The main axes of our research were the following:

        \val{Not sure about the name of this first point}
\begin{itemize}
    \item \textbf{Learning diverse behaviors for planning:}
        Our first two articles focused on learning stochastic policies able to
        explore the environment and discover diverse behaviors. In our first
        article, \emph{Independently Controllable Factors}, we learn policies,
        or options, indexed on a latent factor representation $z$. We were able
        to show that we both learned disentangled representations of the world
        as well as policies that were able to modify these factors. Finally, we
        were able to demonstrate how this representation could be used for
        planning.
        In our second article, \emph{Sequential Monte Carlo Planning}, we
        designed a general purpose planning algorithm that can be used in
        continuous control tasks. Our SMCP algorithm can be viewed as a maximum
        entropy planning algorithm and as such can discover stochastic policies
        discovering diverse solutions.
    \item \textbf{Understanding the role of noise:} The next two contributions
        were more focused on understanding the role of noise during the
        optimization process. In our third article, we analyse the role of the
        interplay between the gradient noise and the local curvature of the loss
        function. Specifically, we show it can impact the optimization speed and
        generalization properties of models trained via maximum likelihood.
        Finally, in our fourth article, we investigate the role of
        \emph{baselines} in policy gradient methods. Baselines are often
        presented as a way to reduce the variance of the gradient estimator
        without affecting the bias. We show that baselines have an effect beyond
        variance reduction directly impact the
        exploration/exploitation trade-off and as such can impact which policies
        are discovered.
\end{itemize}


The work presented in this thesis provides valuable insights into the role of stochasticity in reinforcement learning and optimization algorithms, offering a solid foundation for future research in this area.

By further examining the interplay between noise and learning, we can continue to develop more robust, adaptive, and efficient algorithms that can better handle the challenges of real-world environments. Ultimately, understanding and harnessing the power of stochasticity will bring us closer to achieving the long-term goal of creating intelligent machines capable of interacting with the world and learning from experience, just as humans and animals do.



\section*{Future works}

For future works, we would like to continue to explore the role of stochasticity in reinforcement learning and optimization algorithms. In particular, we would like to investigate the following directions:

\begin{itemize}
    \item Stochasticity and credit assignement? How does the the structure of the
environment can effect learning of the value function?

\item Concentration in high dimensions and the behavior of some objects become
deterministic~\citep{thomas2022role}.
\end{itemize}
 
End goal: being able to understand the properties that affects the behaviors of
large scale models for reinfrocement learning.
 Application to hyperparameter tuning or scaling laws?






