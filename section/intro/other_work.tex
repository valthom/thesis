\chapter*{Non-included works}
We present here some other works I have contributed to but which are not part of the main body of th thesis.

\begin{enumerate}
\item{\bf On the role of overparameterization in off-policy Temporal
Difference learning with linear function
approximation} (\textbf{Valentin Thomas}*). \url{https://openreview.net/forum?id=g-H3oNARs2}, {\em In {\color{blue} NeurIPS 2022}}.\\
This is a theoretical paper studying the convergence of TD methods in high dimension using tools for random graph and matrix theory.

\item{\bf The Role of Baselines in Policy Optimization}
    (with Jincheng Mei*, Wesley Chung, \textbf{Valentin Thomas}, Bo Dai, Csaba
    Szepesvari and Dale Schuurmans).  \url{https://arxiv.org/abs/2301.06276}, {\em In {\color{blue} NeurIPS 2022}}.\\
    This work is a follow-up on our contribution ``Beyond variance reduction: Understanding the true impact of baselines on policy optimization''. It theoretically shows that the true value function as a baseline can explore optimally and guarantee convergence to the optimal policy.
    
\item{\bf Bridging the Gap Between Target Networks and Functional Regularization}
    (Alexandre Pich\'{e}*, \textbf{Valentin Thomas}*, Joseph Marino, Rafael Pardi\~{n}as, Gian Maria Marconi, Christopher Pal, Mohammad Emtiyaz Khan), {\em \url{https://arxiv.org/abs/2210.12282}}.\\
    This work studies the role Target Networks have when learning with Temporal Difference algorithms. We propose a simple variant that can alleviates some unstabilities cause by Target Networks.

% \item{- \bf Probabilistic Planning with Sequential Monte Carlo methods}
%     (\textbf{Valentin Thomas}*, Alexandre Pich\'{e}*, Cyril Ibrahim, Yoshua
%     Bengio and Chris
%     Pal), {\em {\color{olive} Contributed talk at NeurIPS
%     2018 workshop} Infer to Control/published in {\color{blue}ICLR 2019}}.
\item{\bf Planning with Latent Simulated Trajectories} (Alexandre Pich\'{e}*, \textbf{Valentin Thomas}, Cyril Ibrahim, Yoshua Bengio, Julien Cornebise and Chris
    Pal), {\em  In {\color{olive} ICLR 2019 Workshop} on Structure \& Priors in Reinforcement
Learning}.\\
This work is a follow-up on ``Probabilistic Planning with Sequential Monte Carlo Methods'' where we use an iterative algorithm that can smooth out the trajectories obtained by our original paper.

\item{\bf Independently Controllable Features} (Emmanuel Bengio*, \textbf{Valentin Thomas}, Joelle Pineau, Doina Precup and Yoshua Bengio), {\em In {\color{blue}RLDM 2017}}.\\
This work is the precursor to the ``Independently Controllable Factors'' papers and proposes some simple toy experiments with a discrete number of factors showcasing what policy-representation pairs we can learn.
\end{enumerate}
