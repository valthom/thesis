\chapter*{R\'{e}sum\'{e}}
{\color{red} translation, should be reworked once the abstract is finalized}

La plupart des algorithmes modernes d'apprentissage automatique intègrent un certain degré d'aléatoire dans leurs processus, que nous appellerons le bruit, qui peut finalement avoir un impact sur les prédictions du modèle. Dans cette thèse, nous examinons de plus près l'apprentissage et la planification en présence de bruit pour les algorithmes d'apprentissage par renforcement et d'optimisation.

Les deux premiers articles présentés dans ce document se concentrent sur l'apprentissage par renforcement dans un environnement inconnu, et plus précisément sur la façon dont nous pouvons concevoir des algorithmes qui utilisent la stochasticité de leur politique et de l'environnement à leur avantage.
Notre première contribution présentée dans ce document se concentre sur le cadre de l'apprentissage par renforcement non supervisé. Nous montrons comment un agent laissé seul dans un monde inconnu sans but précis peut apprendre quels aspects de l'environnement il peut contrôler indépendamment les uns des autres, ainsi qu'apprendre conjointement une représentation latente démêlée de ces aspects, ou facteurs de variation.
La deuxième contribution se concentre sur la planification dans les tâches de contrôle continu. En présentant l'apprentissage par renforcement comme un problème d'inférence, nous empruntons des outils à la littérature sur le Monte Carlo séquentiel pour concevoir un algorithme efficace et fondé sur la théorie pour la planification probabiliste en utilisant un modèle appris du monde. Nous montrons comment l'agent peut tirer parti de l'incertitude du modèle pour imaginer divers ensembles de solutions.

Les deux contributions suivantes analysent l'impact du bruit de gradient dû à l'échantillonnage dans les algorithmes d'optimisation. 
La troisième contribution examine le rôle du bruit de gradient dans l'estimation par maximum de vraisemblance avec descente de gradient stochastique, en explorant la relation entre la structure du bruit de gradient et la courbure locale sur la généralisation et la vitesse de convergence du modèle. 
Notre quatrième contribution revient au sujet de l'apprentissage par renforcement pour analyser l'impact du bruit d'échantillonnage sur l'algorithme du gradient de politique. Nous constatons que le bruit d'échantillonnage peut avoir un impact significatif sur la dynamique d'optimisation et les politiques découvertes dans l'apprentissage par renforcement on-policy.


  {\bfseries Mots cl\'{e}s\hspace{-3pt}: Apprentissage de repr\'{e}sentations, Contr\^{o}le par Inf\'{e}rence Probabiliste, Apprentisage Profond par Renforcement, Planification.}
                                                                                                                                                            