\chapter*{Notation and acronyms}
In this thesis we will use the convention of denoting matrices with capital bold letters. Vectors and scalars will be denoted by lowercase letters and the distinction will be made clear from context.

\section*{Notation}
\begin{table}[H]%
\begin{center}%
\begin{tabular}{r c p{10cm} }
\toprule
    \multicolumn{3}{l}{\textbf{\underline{Information theory}}} \\
$X$ & $\triangleq$ & Random variable\\
$\E$ & $\triangleq$ & Expectation\\
$H(X)$ & $\triangleq$ & Entropy of $X$\\
$\gI(X,Y)$ & $\triangleq$ & Mutual information between $X$ and $Y$\\
$\KL$ & $\triangleq$ & Kullback-Leibler Divergence \\

\midrule
    \multicolumn{3}{l}{\textbf{\underline{Machine learning and reinforcement
    learning}}} \\
$\theta$ or $\phi$ & $\triangleq$ & Learnable parameters of a model\\
$\mathcal{L}$ & $\triangleq$ & Loss function\\
$\mathcal{J}$ & $\triangleq$ & Objective function to maximize\\
$s$  & $\triangleq$ & State (vector)\\
$a$  & $\triangleq$ & Rction (scalar or vector)\\
$r$  & $\triangleq$ & Reward (scalar) \\
$R$  & $\triangleq$ & Return (scalar) \\
$\traj_{1:T}$ & $\triangleq$ & $\{s_i, a_i\}_{i=1}^T$ Trajectory: sequence of state-action pairs\\
$\pi$ & $\triangleq$ & A policy, i.e a distrubtion over actions given a state\\
$\gamma$ & $\triangleq$ & Discount factor\\
$\lambda$ & $\triangleq$ & Trace-decay parameter for TD($\lambda$)\\
$V^\pi$  & $\triangleq$ & Value function associated to the policy $\pi$\\
$Q^\pi$  & $\triangleq$ & State-action value function associated to the policy $\pi$\\
$\mathcal{O}_t$ & $\triangleq$ & Optimality variable used in control as
    inference\\
$\penv$ & $\triangleq$ & Transition probability of the environment\\
$\rmP$ & $\triangleq$ & Transition matrix of the environment\\
$\pmodel$ & $\triangleq$ & Model of the environment\\
$w_t$ & $\triangleq$ & Importance sampling weight\\ 

\bottomrule
\end{tabular}
\end{center}
\label{tab:notation}
\end{table}

\section*{Acronyms}
\begin{table}[H]%
\begin{center}%
\begin{tabular}{r c{10cm} }
\toprule
\textbf{Acronym} & \textbf{Full name}\\
\midrule
\textbf{ML} & Machine Learning\\
\textbf{DL} & Deep Learning\\
    \textbf{MLE} & Maximum Likelihood Estimation\\
\textbf{RL} & Reinforcement Learning\\
\textbf{DRL} & Deep Reinforcement Learning\\
\textbf{MDP} & Markov Decision Process\\
    \textbf{TD} & Temporal Difference\\
\textbf{ICF} & Independently Controllable Factors\\

\textbf{IS} & Importance Sampling\\
\textbf{MPC} & Model Predictive Control\\
    \textbf{CEM} & Cross Entropy Method\\
\textbf{HMM} & Hidden Markov Model\\
\textbf{MCTS} & Monte Carlo Tree Search\\
\textbf{MCMC} & Markov Chain Monte Carlo\\
    \textbf{SMC} & Sequential Monte Carlo\\
    \textbf{RS} & Random Shooting\\
    \textbf{SAC} & Soft Actor Critic\\
    \textbf{SG} & Stochastic Gradient\\
    \textbf{SGD} & Stochastic Gradient Descent\\
\textbf{PG} & Policy Gradient\\
\textbf{NPG} & Natural Policy Gradient\\
\bottomrule
\end{tabular}
\end{center}
\label{tab:acronyms}
\end{table}


